{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zC1BjWWfyuA"
      },
      "source": [
        "# Clinical Trial Data Processing and Relationship Extraction\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook outlines the steps and processes for processing clinical trial data, extracting relationships, and cleaning the output. The notebook is designed to handle raw clinical trial data, extract meaningful relationships using the Groq API, and prepare the data for further analysis or integration into a graph database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-26T12:29:57.504509Z",
          "iopub.status.busy": "2025-01-26T12:29:57.504229Z",
          "iopub.status.idle": "2025-01-26T12:30:01.952772Z",
          "shell.execute_reply": "2025-01-26T12:30:01.951888Z",
          "shell.execute_reply.started": "2025-01-26T12:29:57.504485Z"
        },
        "trusted": true,
        "id": "breVAIJRfyuC",
        "outputId": "0c5a8ee3-96e0-46fb-d6e9-071ec25bab9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
            "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "\n",
        "# Configure Groq client\n",
        "api_key = \"\"\n",
        "client = Groq(api_key=api_key)\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"\"\n",
        "output_file_path = \"refined3.csv\"\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv(input_file_path, low_memory=False).dropna(how=\"all\")\n",
        "\n",
        "# Merge relevant columns\n",
        "columns_to_merge = [\"Study Title\", \"Primary Outcome Measures\",\n",
        "                   \"Secondary Outcome Measures\", \"criteria\"]\n",
        "df[\"Merged_Content\"] = df[columns_to_merge].apply(\n",
        "    lambda row: \" \\n\".join(row.values.astype(str)), axis=1\n",
        ")\n",
        "\n",
        "# Improved prompt template\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"You are a clinical trial data expert. Extract relationships STRICTLY in this format:\\n\"\n",
        "    \"RELATIONSHIP[TAB]OBJECT\\n\\n\"\n",
        "    \"Relationships to extract:\\n\"\n",
        "    \"- involves: Disease/condition name\\n\"\n",
        "    \"- evaluates: Drug/intervention name\\n\"\n",
        "    \"- measures_primary: Primary outcome (≤5 words)\\n\"\n",
        "    \"- measures_secondary: Secondary outcome (≤5 words)\\n\"\n",
        "    \"- has_criteria: Eligibility criteria (≤5 words)\\n\\n\"\n",
        "    \"Rules:\\n\"\n",
        "    \"1. OBJECT must be ONLY the extracted value - no labels, quotes, or prefixes\\n\"\n",
        "    \"2. Use exact medical terminology from the text\\n\"\n",
        "    \"3. Skip relationships if information is missing\\n\"\n",
        "    \"4. Use TAB separator between relationship and object\\n\\n\"\n",
        "    \"Example output:\\n\"\n",
        "    \"involves\\tAlzheimer's Disease\\n\"\n",
        "    \"evaluates\\tIntravenous Sabirnetug\\n\\n\"\n",
        "    \"Process this clinical trial data:\\n{content}\"\n",
        ")\n",
        "\n",
        "def extract_relationships(content):\n",
        "    \"\"\"Process content and return list of (relationship, object) tuples\"\"\"\n",
        "    if pd.isnull(content) or not content.strip():\n",
        "        return []\n",
        "\n",
        "    # Truncate long content\n",
        "    content = str(content)\n",
        "    if len(content) > 2000:\n",
        "        content = content[:2000] + \"... [TRUNCATED]\"\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gemma2-9b-it\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": PROMPT_TEMPLATE.format(content=content)\n",
        "            }],\n",
        "            temperature=0.3,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        response = completion.choices[0].message.content\n",
        "\n",
        "        # Parse response lines\n",
        "        relationships = []\n",
        "        for line in response.splitlines():\n",
        "            if \"\\t\" in line:\n",
        "                rel, obj = line.split(\"\\t\", 1)\n",
        "                rel = rel.strip().lower()\n",
        "                obj = obj.strip(\" '\\\"\")  # Clean quotes\n",
        "\n",
        "                # Validate relationships\n",
        "                if rel in {'involves', 'evaluates', 'measures_primary',\n",
        "                          'measures_secondary', 'has_criteria'} and obj:\n",
        "                    relationships.append((rel, obj))\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing content: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Initialize output file\n",
        "with open(output_file_path, 'w') as f:\n",
        "    f.write(\"Subject,Relationship,Object\\n\")\n",
        "\n",
        "# Process rows and write results\n",
        "for index, row in df.iterrows():\n",
        "    subject_id = row.get(\"NCT Number\", f\"ROW_{index}\")\n",
        "    relationships = extract_relationships(row[\"Merged_Content\"])\n",
        "\n",
        "    if relationships:\n",
        "        with open(output_file_path, 'a') as f:\n",
        "            for rel, obj in relationships:\n",
        "                f.write(f\"{subject_id},{rel},{obj}\\n\")\n",
        "\n",
        "    print(f\"Processed row {index} - Extracted {len(relationships)} relationships\")\n",
        "\n",
        "print(f\"Processing complete. Results saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "RbL3SnYUgW-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-01-26T01:49:51.332553Z",
          "iopub.status.busy": "2025-01-26T01:49:51.332250Z",
          "iopub.status.idle": "2025-01-26T01:49:51.344013Z",
          "shell.execute_reply": "2025-01-26T01:49:51.343364Z",
          "shell.execute_reply.started": "2025-01-26T01:49:51.332505Z"
        },
        "trusted": true,
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "X7m9PDYxfyuF"
      },
      "outputs": [],
      "source": [
        "## Cleaning the csv for extra columns\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/kaggle/working/refined3.csv'\n",
        "output_file = 'part3_cleaned.csv'  # Save to a different file to avoid overwriting prematurely\n",
        "\n",
        "# Process the CSV to keep only the first three columns\n",
        "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    for row in reader:\n",
        "        # Check if the row has at least three columns before slicing\n",
        "        if len(row) >= 3:\n",
        "            writer.writerow(row[:3])\n",
        "\n",
        "# Read the updated CSV file\n",
        "try:\n",
        "    df = pd.read_csv(output_file)\n",
        "\n",
        "    # Get the value counts for the 'Object' column\n",
        "    object_value_counts = df['Object'].value_counts()\n",
        "\n",
        "    # Display the value counts\n",
        "    print(object_value_counts)\n",
        "\n",
        "    # Save the value counts to a CSV file\n",
        "    object_value_counts.to_csv('Object_Value_Counts2.csv', header=['Count'])\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"The cleaned CSV file is empty or invalid. Please check the input file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "sZxy9xStfyuF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6550712,
          "sourceId": 10585189,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}