{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr4uWqZB3KMC"
      },
      "source": [
        "# Detect Same / Nearly Same Nodes for Merger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxQHhjpy3ggn"
      },
      "source": [
        "Same or Nearly Same Nodes (eg: Singular and Plural forms of same entity or verb vs noun) need to be merged efficiently as they represent the same thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqZu7TuHz05A"
      },
      "source": [
        "This ipynb is designed to generate GPU-accelerated embeddings for object nodes using a pre-trained transformer model (Stella 1.5B). The embeddings are computed in batches for efficient memory management and saved for downstream usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtagK_87zoIW"
      },
      "outputs": [],
      "source": [
        "#First Execute This\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# Setup logging to print to console only\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# GPU Configuration\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "def get_embeddings(texts, tokenizer, model, batch_size=256):\n",
        "    \"\"\"\n",
        "    Generate GPU-accelerated embeddings for a list of texts.\n",
        "    Processes texts in batches to optimize memory usage.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=512\n",
        "            ).to(device)\n",
        "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
        "            # Use the mean of the last hidden state as the embedding\n",
        "            batch_embeddings = outputs.hidden_states[-1].mean(dim=1)  # Shape: (batch_size, hidden_size)\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "            logger.debug(f\"Processed batch {(i // batch_size) + 1}\")\n",
        "    embeddings = np.vstack(embeddings)  # Shape: (num_texts, hidden_size)\n",
        "    return embeddings\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load tokenizer and model with GPU support\n",
        "        logger.info(\"Loading tokenizer and model\")\n",
        "        model_name = \"NovaSearch/stella_en_1.5B_v5\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
        "        logger.info(\"Tokenizer and model loaded successfully\")\n",
        "\n",
        "        # Configuration\n",
        "        csv_path = \"/kaggle/input/objectcount2/Object_Value_Counts2.csv\"  # Update this path as needed\n",
        "        embeddings_output_path = \"object_embeddings.npy\"\n",
        "        objects_output_path = \"objects_list.npy\"\n",
        "        embedding_batch_size = 256  # Adjust based on GPU memory\n",
        "\n",
        "        # Read CSV\n",
        "        logger.info(f\"Reading CSV from {csv_path}\")\n",
        "        data = pd.read_csv(csv_path)\n",
        "        if \"Object\" not in data.columns:\n",
        "            logger.error(\"'Object' column missing in CSV\")\n",
        "            raise ValueError(\"'Object' column missing in CSV\")\n",
        "        objects = data[\"Object\"].astype(str).tolist()  # Ensure all objects are strings\n",
        "        logger.info(f\"Loaded {len(objects)} objects from CSV\")\n",
        "\n",
        "        # Generate embeddings for all objects\n",
        "        logger.info(\"Generating embeddings for all objects\")\n",
        "        embeddings = get_embeddings(objects, tokenizer, model, batch_size=embedding_batch_size)\n",
        "        logger.info(\"Embeddings generated successfully\")\n",
        "\n",
        "        # Save embeddings and objects\n",
        "        logger.info(f\"Saving embeddings to {embeddings_output_path}\")\n",
        "        np.save(embeddings_output_path, embeddings)\n",
        "        logger.info(f\"Saving objects list to {objects_output_path}\")\n",
        "        np.save(objects_output_path, np.array(objects))\n",
        "        logger.info(\"Embeddings and objects saved successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.exception(\"An error occurred during embedding computation\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mkIyC2C2-6H"
      },
      "source": [
        "This script enhances the dataset by finding similar objects based on precomputed embeddings using FAISS (Facebook AI Similarity Search). It identifies objects with high similarity, augments the dataset, and saves the results to a CSV file for merging duplicate nodes in neo4j."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfsqzhdP1FJ9"
      },
      "outputs": [],
      "source": [
        "# Then Execute This\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import faiss  # Facebook AI Similarity Search\n",
        "import csv  # Import the csv module for quoting constants\n",
        "\n",
        "# GPU Configuration\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def build_faiss_index(embeddings, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Build a FAISS index for the given embeddings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dimension = embeddings.shape[1]\n",
        "        print(\"Building FAISS index on CPU...\")\n",
        "        index = faiss.IndexFlatIP(dimension)  # Using Inner Product (cosine similarity if vectors are normalized)\n",
        "\n",
        "        # Skip GPU indexing to prevent OOM\n",
        "        # Uncomment the following lines if you resolve the OOM issue and want to use GPU FAISS\n",
        "        # if use_gpu and faiss.get_num_gpus() > 0:\n",
        "        #     print(\"Moving FAISS index to GPU...\")\n",
        "        #     res = faiss.StandardGpuResources()\n",
        "        #     index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "\n",
        "        # Normalize embeddings to unit length for cosine similarity\n",
        "        print(\"Normalizing embeddings...\")\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        index.add(embeddings)\n",
        "        print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        print(\"Error in build_faiss_index:\", e)\n",
        "        raise\n",
        "\n",
        "def find_similar_objects(embeddings, index, objects, similarity_threshold=0.7, top_k=100):\n",
        "    \"\"\"\n",
        "    For each embedding, find similar objects with similarity above the threshold.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Searching for similar objects using FAISS...\")\n",
        "        faiss.normalize_L2(embeddings)  # Ensure embeddings are normalized\n",
        "\n",
        "        # Perform similarity search\n",
        "        # FAISS searches for top_k similar vectors; we'll filter them based on the threshold\n",
        "        similarities, indices = index.search(embeddings, top_k)\n",
        "\n",
        "        similar_objects_list = []\n",
        "        total_objects = len(objects)\n",
        "        for idx, (sim, ind) in enumerate(zip(similarities, indices)):\n",
        "            # Filter out self-match and apply the similarity threshold\n",
        "            similar = []\n",
        "            for score, index_ in zip(sim, ind):\n",
        "                if index_ == idx:\n",
        "                    continue  # Skip self\n",
        "                if score < similarity_threshold:\n",
        "                    continue\n",
        "                similar.append(objects[index_])\n",
        "            similar_objects_list.append(similar)\n",
        "\n",
        "            # Print progress every 1000 objects\n",
        "            if (idx + 1) % 1000 == 0 or (idx + 1) == total_objects:\n",
        "                print(f\"Processed {idx + 1} / {total_objects} objects.\")\n",
        "\n",
        "        print(\"Similarity search completed.\")\n",
        "        return similar_objects_list\n",
        "    except Exception as e:\n",
        "        print(\"Error in find_similar_objects:\", e)\n",
        "        raise\n",
        "\n",
        "def serialize_list_with_double_quotes(lst):\n",
        "    \"\"\"\n",
        "    Serialize a list of strings ensuring all elements are wrapped in double quotes.\n",
        "\n",
        "    Args:\n",
        "        lst (list of str): The list to serialize.\n",
        "\n",
        "    Returns:\n",
        "        str: The serialized list as a string with all elements in double quotes.\n",
        "    \"\"\"\n",
        "    # Escape any existing double quotes in the strings\n",
        "    escaped_lst = [s.replace('\"', '\\\\\"') for s in lst]\n",
        "    # Wrap each string with double quotes\n",
        "    quoted_lst = ['\"{}\"'.format(s) for s in escaped_lst]\n",
        "    # Join into a list-like string\n",
        "    return '[' + ', '.join(quoted_lst) + ']'\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Clear GPU cache (if using GPU)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Configuration\n",
        "        embeddings_input_path = \"object_embeddings.npy\"\n",
        "        objects_input_path = \"objects_list.npy\"\n",
        "        csv_path = \"/kaggle/input/objectcount2/Object_Value_Counts2.csv\"  # Update this path as needed\n",
        "        output_path = \"filtered_results_with_similars.csv\"\n",
        "        similarity_threshold = 0.8\n",
        "        top_k = 100  # Maximum number of similar objects to retrieve per object\n",
        "\n",
        "        # Load embeddings and objects\n",
        "        print(f\"Loading embeddings from {embeddings_input_path}...\")\n",
        "        embeddings = np.load(embeddings_input_path)\n",
        "        print(f\"Loading objects list from {objects_input_path}...\")\n",
        "        objects = np.load(objects_input_path).tolist()\n",
        "        print(f\"Loaded {len(objects)} objects and their embeddings.\")\n",
        "\n",
        "        # Build FAISS index\n",
        "        index = build_faiss_index(embeddings, use_gpu=False)\n",
        "\n",
        "        # Find similar objects\n",
        "        similar_objects_list = find_similar_objects(\n",
        "            embeddings, index, objects, similarity_threshold=similarity_threshold, top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Read original CSV\n",
        "        print(f\"Reading CSV from {csv_path}...\")\n",
        "        data = pd.read_csv(csv_path)\n",
        "        if \"Object\" not in data.columns:\n",
        "            print(\"Error: 'Object' column missing in CSV.\")\n",
        "            raise ValueError(\"'Object' column missing in CSV\")\n",
        "        print(\"CSV loaded successfully.\")\n",
        "\n",
        "        # Serialize the similar objects lists with double quotes\n",
        "        print(\"Serializing similar objects with consistent double quotes...\")\n",
        "        serialized_similar_objects = [serialize_list_with_double_quotes(lst) for lst in similar_objects_list]\n",
        "\n",
        "        # Add the serialized similar objects as a new column\n",
        "        print(\"Adding similar objects to the DataFrame...\")\n",
        "        data[\"Similar_Objects\"] = serialized_similar_objects\n",
        "\n",
        "        # Save the final DataFrame to CSV with proper quoting\n",
        "        print(f\"Saving the final DataFrame to {output_path}...\")\n",
        "        data.to_csv(output_path, index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "        print(f\"Saved filtered results with similar objects to {output_path}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during similarity search and CSV augmentation:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5a6DNqM-XzI"
      },
      "source": [
        "This script processes a dataset with a column containing similar objects to remove duplicates. It ensures that an object does not appear in its own \"Similar_Objects\" list and saves the cleaned dataset for further use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O63O80en-SgX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/filtered_results_with_similars_final.csv'  # Update with the correct path to your file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Similar_Objects' column to lowercase for uniformity and search\n",
        "data['Similar_Objects'] = data['Similar_Objects'].str.lower()\n",
        "\n",
        "# Remove duplicates by checking if the object is already present in any of the 'Similar_Objects' values\n",
        "unique_data = data[~data.apply(lambda row: any(row['Object'].lower() in obj for obj in eval(row['Similar_Objects'])), axis=1)]\n",
        "\n",
        "# Save the cleaned file\n",
        "output_file_path = 'cleaned_file.csv'  # Update with the desired save path\n",
        "unique_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Duplicate rows removed and cleaned data saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgyagkp-msH"
      },
      "source": [
        "This script processes two CSV files: a cleaned dataset and a merged dataset. It updates the Object column in the merged dataset based on the Similar_Objects from the cleaned dataset. Additionally, it handles missing values (NaN) and saves the updated dataset to a new file for finally creating Nodes in Neo4j."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qca4hx8a-VHq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "cleaned_file_path = '/content/cleaned_file.csv'  # Update with the actual cleaned file path\n",
        "merged_file_path = '/content/merged.csv'        # Update with the actual merged file path\n",
        "output_file_path = 'output_file.csv'   # Update with the desired output path\n",
        "\n",
        "# Load the cleaned and merged CSV files\n",
        "cleaned_data = pd.read_csv(cleaned_file_path)\n",
        "merged_data = pd.read_csv(merged_file_path)\n",
        "\n",
        "# Normalize the Similar_Objects column in the cleaned file\n",
        "cleaned_data['Similar_Objects'] = cleaned_data['Similar_Objects'].apply(eval)  # Convert to list\n",
        "\n",
        "# Create a mapping dictionary from Similar_Objects to Object\n",
        "mapping = {}\n",
        "for _, row in cleaned_data.iterrows():\n",
        "    for similar_object in row['Similar_Objects']:\n",
        "        mapping[similar_object.lower()] = row['Object']\n",
        "\n",
        "# Fill NaN values in the Object column with a placeholder (e.g., \"Unknown\")\n",
        "merged_data['Object'] = merged_data['Object'].fillna(\"None\")\n",
        "\n",
        "# Update the Object column in the merged CSV\n",
        "def update_object(obj):\n",
        "    # Check if obj is a string, otherwise return it as is\n",
        "    if isinstance(obj, str):\n",
        "        return mapping[obj.lower()] if obj.lower() in mapping else obj\n",
        "    return obj\n",
        "\n",
        "merged_data['Object'] = merged_data['Object'].apply(update_object)\n",
        "\n",
        "# Save the updated merged file\n",
        "merged_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Object column updated, NaN values handled, and file saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1. **CSV File Upload**:\n",
        "   - Place the CSV file (e.g., `output_file.csv`) in a directory accessible to Neo4j (e.g., `/import`).\n",
        "   - Update the `neo4j.conf` file to include:\n",
        "     ```\n",
        "     dbms.directories.import=/path/to/your/import/directory\n",
        "     ```\n",
        "\n",
        "2. **Neo4j Server**:\n",
        "   - Ensure the Neo4j server is running and accessible.\n",
        "   - Use the correct URI, username, and password for authentication.\n",
        "\n",
        "3. **Python Environment**:\n",
        "   - Install the required libraries:\n",
        "     ```bash\n",
        "     pip install neo4j pandas\n",
        "     ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "# Connection details\n",
        "URI = \"neo4j://38.242.232.192:7687\"\n",
        "AUTH = (\"nest\", \"neurons-newbie\")\n",
        "\n",
        "# Query to load the CSV file into Neo4j\n",
        "query_load_csv = \"\"\"\n",
        "LOAD CSV WITH HEADERS FROM 'file:///path/to/your/output_file.csv' AS row\n",
        "CALL {\n",
        "    WITH row\n",
        "    MERGE (t:SubjectNode { name: row['Subject'] })\n",
        "    MERGE (o:ObjectNode { name: COALESCE(row['Object'], 'None') })\n",
        "    MERGE (t)-[r:RELATIONSHIP { name: toUpper(row['Relationship']) }]-(o)\n",
        "} IN TRANSACTIONS;\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    # Establish connection to the Neo4j database\n",
        "    with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
        "        with driver.session() as session:\n",
        "            print(\"Starting CSV data load into Neo4j...\")\n",
        "            \n",
        "            # Execute the query to load CSV data\n",
        "            session.run(query_load_csv)\n",
        "            print(\"CSV data has been successfully loaded into Neo4j!\")\n",
        "\n",
        "            # Verify the number of nodes and relationships loaded\n",
        "            query_nodes = \"MATCH (n) RETURN COUNT(n) AS TotalNodes;\"\n",
        "            query_relationships = \"MATCH ()-[r]->() RETURN COUNT(r) AS TotalRelationships;\"\n",
        "\n",
        "            result_nodes = session.run(query_nodes)\n",
        "            for record in result_nodes:\n",
        "                print(\"Total Nodes in the Database:\", record[\"TotalNodes\"])\n",
        "\n",
        "            result_relationships = session.run(query_relationships)\n",
        "            for record in result_relationships:\n",
        "                print(\"Total Relationships in the Database:\", record[\"TotalRelationships\"])\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
