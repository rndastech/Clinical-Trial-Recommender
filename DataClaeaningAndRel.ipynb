{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configure the Groq client\n",
    "from groq import Groq\n",
    "\n",
    "# Set your Groq API key\n",
    "api_key = \"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\"  # Replace with your actual API key\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your Groq API key in the 'api_key' variable.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# File to read and write\n",
    "input_file_path = \"/kaggle/input/filtereddata/filtered_data.csv\"  # Input file path\n",
    "output_file_path = \"/kaggle/working/outputCsvUpd2.csv\"  # Output file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(input_file_path, low_memory=False)\n",
    "\n",
    "# Remove unintended empty rows\n",
    "df = df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "# Define columns to clean\n",
    "columns_to_clean = [\"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\", \"criteria\"]\n",
    "\n",
    "# Merge all columns into a single column for processing\n",
    "def merge_columns(row):\n",
    "    return \" \\n\".join([str(row[col]) for col in columns_to_clean if col in row and not pd.isnull(row[col])])\n",
    "\n",
    "df[\"Merged_Content\"] = df.apply(merge_columns, axis=1)\n",
    "\n",
    "# Function to clean and extract relationships using Groq\n",
    "def extract_relationships(content, row_index):\n",
    "    if pd.isnull(content):\n",
    "        return [None, None, None, None, None]\n",
    "\n",
    "    # Truncate content if it exceeds a certain limit to avoid API errors\n",
    "    max_length = 2000  # Adjust the limit as needed\n",
    "    if len(content) > max_length:\n",
    "        content = content[:max_length] + \"...\"\n",
    "\n",
    "    prompt = (\n",
    "        \"The following text is a merged representation of a clinical trials dataset row. \"\n",
    "        \"Extract the following relationships from it in the format: \\n\"\n",
    "        \"Subject, Relationship, Object (one per line): \\n\"\n",
    "        \"1. involves (Disease): \\n\"\n",
    "        \"2. evaluates (Drug): \\n\"\n",
    "        \"3. measures_primary (Primary Outcome): \\n\"\n",
    "        \"4. measures_secondary (Secondary Outcome): \\n\"\n",
    "        \"5. has_criteria (Criteria): \\n\"\n",
    "        \"Ensure each extracted Object contains a maximum of 5 words. \\n\"\n",
    "        f\"\\n{content}\"\n",
    "    )\n",
    "\n",
    "    # Call the Groq API to get a response\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gemma2-9b-it\",  # Use the appropriate model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract the response text from the correct object\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    # Parse the response text to extract the relationships\n",
    "    extracted = [None, None, None, None, None]\n",
    "    for line in response.splitlines():\n",
    "        if \"involves\" in line:\n",
    "            extracted[0] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"evaluates\" in line:\n",
    "            extracted[1] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"measures_primary\" in line:\n",
    "            extracted[2] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"measures_secondary\" in line:\n",
    "            extracted[3] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"has_criteria\" in line:\n",
    "            extracted[4] = line.split(\",\", 2)[-1].strip()\n",
    "\n",
    "    print(f\"Row {row_index} processed.\")  # Log row completion\n",
    "    return extracted\n",
    "\n",
    "# Ensure output file is cleared or created\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(\"Subject,Relationship,Object\\n\")\n",
    "\n",
    "# Process each row, extract relationships, and write to the CSV immediately\n",
    "for index, row in df.iloc[129:].iterrows():\n",
    "    extracted = extract_relationships(row[\"Merged_Content\"], index)\n",
    "    subject = row.get(\"NCT Number\", f\"Row {index}\")  # Replace \"Study ID\" with appropriate column name for Subject\n",
    "\n",
    "    # Write each relationship as a separate row in the output file\n",
    "    with open(output_file_path, 'a') as f:\n",
    "        f.write(f\"{subject},involves,{extracted[0]}\\n\")\n",
    "        f.write(f\"{subject},evaluates,{extracted[1]}\\n\")\n",
    "        f.write(f\"{subject},measures_primary,{extracted[2]}\\n\")\n",
    "        f.write(f\"{subject},measures_secondary,{extracted[3]}\\n\")\n",
    "        f.write(f\"{subject},has_criteria,{extracted[4]}\\n\")\n",
    "\n",
    "    print(f\"Row {index} written to file.\")\n",
    "\n",
    "print(f\"Data extraction complete. Updated data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import fuzz, process  # For fuzzy string matching\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"/kaggle/input/outputcsvupd/outputCsvUpd.csv\"  # Input dataset\n",
    "output_file_path = \"/kaggle/working/processed_outputCSVUpd.csv\"   # Output dataset\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Custom lemmatization function\n",
    "def custom_lemmatizer(word):\n",
    "    \"\"\"\n",
    "    A lightweight custom lemmatizer for basic normalization.\n",
    "    This can handle common cases like plural/singular forms and known transformations.\n",
    "    \"\"\"\n",
    "    # Basic singular/plural transformations\n",
    "    word = word.lower()\n",
    "    if word.endswith('ies'):\n",
    "        word = word[:-3] + 'y'\n",
    "    elif word.endswith('es'):\n",
    "        word = word[:-2]\n",
    "    elif word.endswith('s') and len(word) > 1:\n",
    "        word = word[:-1]\n",
    "    \n",
    "    # Additional custom rules (add as needed)\n",
    "    lemma_mapping = {\n",
    "        \"alzheimer's\": \"alzheimer\",\n",
    "        \"diseases\": \"disease\",\n",
    "        \"diabetics\": \"diabetes\",\n",
    "    }\n",
    "    \n",
    "    return lemma_mapping.get(word, word)\n",
    "\n",
    "# Preprocessing function for the Object column\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text) or text.lower() == \"none\":\n",
    "        return text\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Apply custom lemmatization to each word\n",
    "    text = \" \".join([custom_lemmatizer(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "# Function to normalize text using fuzzy matching\n",
    "def normalize_text(text, reference_list, threshold=85):\n",
    "    \"\"\"\n",
    "    Normalize text using fuzzy matching against a reference list.\n",
    "    Args:\n",
    "        text (str): The text to normalize.\n",
    "        reference_list (list): A list of reference strings to match against.\n",
    "        threshold (int): The minimum similarity score for a match.\n",
    "    Returns:\n",
    "        str: The normalized text if a match is found, otherwise the original text.\n",
    "    \"\"\"\n",
    "    match_result = process.extractOne(text, reference_list, scorer=fuzz.ratio)\n",
    "    if match_result and match_result[1] >= threshold:\n",
    "        return match_result[0]  # Return the matched text\n",
    "    return text  # Return the original text if no match is above the threshold\n",
    "\n",
    "# Build a reference list from the Object column\n",
    "reference_list = df[\"Object\"].dropna().unique()\n",
    "\n",
    "# Ensure output file is cleared or created\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(\",\".join(df.columns) + \"\\n\")  # Write headers\n",
    "\n",
    "# Process each row, apply preprocessing and normalization, and write to the file immediately\n",
    "for index, row in df.iterrows():\n",
    "    original_text = row[\"Object\"]\n",
    "    preprocessed_text = preprocess_text(original_text)\n",
    "    normalized_text = normalize_text(preprocessed_text, reference_list)\n",
    "    row[\"Object\"] = normalized_text\n",
    "    with open(output_file_path, 'a') as f:\n",
    "        f.write(\",\".join(map(str, row.values)) + \"\\n\")\n",
    "    print(f\"Row {index} processed and written to file: {normalized_text}\")\n",
    "\n",
    "print(f\"Processing complete. Processed dataset saved to {output_file_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
