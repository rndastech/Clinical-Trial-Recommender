{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configure the Groq client\n",
    "from groq import Groq\n",
    "\n",
    "# Set your Groq API key\n",
    "api_key = \"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\"  # Replace with your actual API key\n",
    "if not api_key:\n",
    "    raise ValueError(\"Please set your Groq API key in the 'api_key' variable.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# File to read and write\n",
    "input_file_path = \"/content/trials - Copy.csv\"  # Input file path\n",
    "output_file_path = \"refined.csv\"  # Output file path\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(input_file_path, low_memory=False)\n",
    "\n",
    "# Remove unintended empty rows\n",
    "df = df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "# Define columns to clean\n",
    "columns_to_clean = [\"Study Title\", \"Primary Outcome Measures\", \"Secondary Outcome Measures\", \"criteria\"]\n",
    "\n",
    "# Merge all columns into a single column for processing\n",
    "def merge_columns(row):\n",
    "    return \" \\n\".join([str(row[col]) for col in columns_to_clean if col in row and not pd.isnull(row[col])])\n",
    "\n",
    "df[\"Merged_Content\"] = df.apply(merge_columns, axis=1)\n",
    "\n",
    "# Function to clean and extract relationships using Groq\n",
    "def extract_relationships(content, row_index):\n",
    "    if pd.isnull(content):\n",
    "        return [None, None, None, None, None]\n",
    "\n",
    "    # Truncate content if it exceeds a certain limit to avoid API errors\n",
    "    max_length = 2000  # Adjust the limit as needed\n",
    "    if len(content) > max_length:\n",
    "        content = content[:max_length] + \"...\"\n",
    "\n",
    "    \n",
    "\n",
    "    prompt = (\n",
    "          \"The following text is a merged representation of a clinical trials dataset row. \"\n",
    "          \"Extract only valid and concise relationships from it in the exact format below:\\n\"\n",
    "          \"'Subject', 'Relationship', 'Object' (one per line):\\n\\n\"\n",
    "          \"'involves (Disease)': 'The name of the primary disease or condition being studied \"\n",
    "          '(e.g., \"Diabetes\", \"Hypertension\").\\'\\n\\n'\n",
    "          \"'evaluates (Drug)': 'The name of the drug, therapy, or intervention being evaluated. \"\n",
    "          '(e.g., \"Aspirin\", \"Gene Therapy\"). \\'\\n\\n'\n",
    "          \"'measures_primary (Primary Outcome)': 'The main outcome or result the trial is designed \"\n",
    "          'to measure, in up to 5 words (e.g., \"Reduction in blood pressure\").\\'\\n\\n'\n",
    "          \"'measures_secondary (Secondary Outcome)': 'Additional outcomes or results measured in the trial, \"\n",
    "          'in up to 5 words (e.g., \"Improved insulin sensitivity\").\\'\\n\\n'\n",
    "          \"'has_criteria (Criteria)': 'Key eligibility criteria for participants in up to 5 words \"\n",
    "          '(e.g., \"Ages 18-65, no diabetes\").\\'\\n\\n'\n",
    "          \"Rules:\\n\\n\"\n",
    "          \"- 'Assume you are a biomedical expert with a deep understanding of clinical trial terminology.'\\n\"\n",
    "          # \"- 'If the information for an object cannot be found or is ambiguous, write \\\"None\\\" instead of guessing.'\\n\"\n",
    "          \"- 'Objects must be relevant, concise, and meaningful, with a maximum of 5 words. Avoid long phrases, generic terms, or irrelevant outputs.'\\n\\n\"\n",
    "          \"'Content to analyze\\n'\"\n",
    "          f\"\\n{content}\"\n",
    ")\n",
    "\n",
    "    # Call the Groq API to get a response\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gemma2-9b-it\",  # Use the appropriate model\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=1,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    # Extract the response text from the correct object\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    # Parse the response text to extract the relationships\n",
    "    extracted = [None, None, None, None, None]\n",
    "    for line in response.splitlines():\n",
    "        if \"involves\" in line:\n",
    "            extracted[0] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"evaluates\" in line:\n",
    "            extracted[1] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"measures_primary\" in line:\n",
    "            extracted[2] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"measures_secondary\" in line:\n",
    "            extracted[3] = line.split(\",\", 2)[-1].strip()\n",
    "        elif \"has_criteria\" in line:\n",
    "            extracted[4] = line.split(\",\", 2)[-1].strip()\n",
    "\n",
    "    print(f\"Row {row_index} processed.\")  # Log row completion\n",
    "    return extracted\n",
    "\n",
    "# Ensure output file is cleared or created\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(\"Subject,Relationship,Object\\n\")\n",
    "\n",
    "# Process each row, extract relationships, and write to the CSV immediately\n",
    "for index, row in df.iloc[0:].iterrows():\n",
    "    extracted = extract_relationships(row[\"Merged_Content\"], index)\n",
    "    subject = row.get(\"NCT Number\", f\"Row {index}\")  # Replace \"Study ID\" with appropriate column name for Subject\n",
    "\n",
    "    # Write each relationship as a separate row in the output file\n",
    "    with open(output_file_path, 'a') as f:\n",
    "        f.write(f\"{subject},involves,{extracted[0]}\\n\")\n",
    "        f.write(f\"{subject},evaluates,{extracted[1]}\\n\")\n",
    "        f.write(f\"{subject},measures_primary,{extracted[2]}\\n\")\n",
    "        f.write(f\"{subject},measures_secondary,{extracted[3]}\\n\")\n",
    "        f.write(f\"{subject},has_criteria,{extracted[4]}\\n\")\n",
    "\n",
    "    print(f\"Row {index} written to file.\")\n",
    "\n",
    "print(f\"Data extraction complete. Updated data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## OM God's Code\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "\n",
    "# Configure Groq client\n",
    "api_key = \"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\"  # Replace with your actual API key\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"/content/data_300.csv\"\n",
    "output_file_path = \"refined4.csv\"\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(input_file_path, low_memory=False).dropna(how=\"all\")\n",
    "\n",
    "# Merge relevant columns\n",
    "columns_to_merge = [\"Study Title\", \"Primary Outcome Measures\", \n",
    "                   \"Secondary Outcome Measures\", \"criteria\"]\n",
    "df[\"Merged_Content\"] = df[columns_to_merge].apply(\n",
    "    lambda row: \" \\n\".join(row.values.astype(str)), axis=1\n",
    ")\n",
    "\n",
    "# Improved prompt template\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a clinical trial data expert. Extract relationships STRICTLY in this format:\\n\"\n",
    "    \"RELATIONSHIP[TAB]OBJECT\\n\\n\"\n",
    "    \"Relationships to extract:\\n\"\n",
    "    \"- involves: Disease/condition name\\n\"\n",
    "    \"- evaluates: Drug/intervention name\\n\"\n",
    "    \"- measures_primary: Primary outcome (≤5 words)\\n\"\n",
    "    \"- measures_secondary: Secondary outcome (≤5 words)\\n\"\n",
    "    \"- has_criteria: Eligibility criteria (≤5 words)\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. OBJECT must be ONLY the extracted value - no labels, quotes, or prefixes\\n\"\n",
    "    \"2. Use exact medical terminology from the text\\n\"\n",
    "    \"3. Skip relationships if information is missing\\n\"\n",
    "    \"4. Use TAB separator between relationship and object\\n\\n\"\n",
    "    \"Example output:\\n\"\n",
    "    \"involves\\tAlzheimer's Disease\\n\"\n",
    "    \"evaluates\\tIntravenous Sabirnetug\\n\\n\"\n",
    "    \"Process this clinical trial data:\\n{content}\"\n",
    ")\n",
    "\n",
    "def extract_relationships(content):\n",
    "    \"\"\"Process content and return list of (relationship, object) tuples\"\"\"\n",
    "    if pd.isnull(content) or not content.strip():\n",
    "        return []\n",
    "\n",
    "    # Truncate long content\n",
    "    content = str(content)\n",
    "    if len(content) > 2000:\n",
    "        content = content[:2000] + \"... [TRUNCATED]\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gemma2-9b-it\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": PROMPT_TEMPLATE.format(content=content)\n",
    "            }],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        \n",
    "        # Parse response lines\n",
    "        relationships = []\n",
    "        for line in response.splitlines():\n",
    "            if \"\\t\" in line:\n",
    "                rel, obj = line.split(\"\\t\", 1)\n",
    "                rel = rel.strip().lower()\n",
    "                obj = obj.strip(\" '\\\"\")  # Clean quotes\n",
    "                \n",
    "                # Validate relationships\n",
    "                if rel in {'involves', 'evaluates', 'measures_primary', \n",
    "                          'measures_secondary', 'has_criteria'} and obj:\n",
    "                    relationships.append((rel, obj))\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Initialize output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(\"Subject,Relationship,Object\\n\")\n",
    "\n",
    "# Process rows and write results\n",
    "for index, row in df.iterrows():\n",
    "    subject_id = row.get(\"NCT Number\", f\"ROW_{index}\")\n",
    "    relationships = extract_relationships(row[\"Merged_Content\"])\n",
    "    \n",
    "    if relationships:\n",
    "        with open(output_file_path, 'a') as f:\n",
    "            for rel, obj in relationships:\n",
    "                f.write(f\"{subject_id},{rel},{obj}\\n\")\n",
    "    \n",
    "    print(f\"Processed row {index} - Extracted {len(relationships)} relationships\")\n",
    "\n",
    "print(f\"Processing complete. Results saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning the csv for extra columns\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file = '/content/refined4 (1).csv'\n",
    "output_file = '/content/refined4_cleaned.csv'  # Save to a different file to avoid overwriting prematurely\n",
    "\n",
    "# Process the CSV to keep only the first three columns\n",
    "with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    for row in reader:\n",
    "        # Check if the row has at least three columns before slicing\n",
    "        if len(row) >= 3:\n",
    "            writer.writerow(row[:3])\n",
    "\n",
    "# Read the updated CSV file\n",
    "try:\n",
    "    df = pd.read_csv(output_file)\n",
    "\n",
    "    # Get the value counts for the 'Object' column\n",
    "    object_value_counts = df['Object'].value_counts()\n",
    "\n",
    "    # Display the value counts\n",
    "    print(object_value_counts)\n",
    "\n",
    "    # Save the value counts to a CSV file\n",
    "    object_value_counts.to_csv('Object_Value_Counts2.csv', header=['Count'])\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"The cleaned CSV file is empty or invalid. Please check the input file.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
