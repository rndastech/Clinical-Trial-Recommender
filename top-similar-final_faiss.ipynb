{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10521853,"sourceType":"datasetVersion","datasetId":6511986}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:05:56.130555Z","iopub.execute_input":"2025-01-23T16:05:56.130875Z","iopub.status.idle":"2025-01-23T16:05:56.139684Z","shell.execute_reply.started":"2025-01-23T16:05:56.130847Z","shell.execute_reply":"2025-01-23T16:05:56.138804Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/embeddingstrials/embeddings_output (2).csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import faiss\nimport numpy as np\nimport pandas as pd\n\ndef create_optimized_index(embeddings, embedding_dimension=768, n_clusters=200):\n    \"\"\"Optimized FAISS index creation with improved parameters\"\"\"\n    # Convert to contiguous array FIRST before normalization\n    embeddings = np.ascontiguousarray(embeddings.astype('float32'))\n    faiss.normalize_L2(embeddings)  # Normalization after ensuring contiguity\n    \n    # Enhanced clustering parameters\n    n_clusters = min(int(np.sqrt(len(embeddings)) * 6), 4096)\n    \n    # GPU configuration\n    if faiss.get_num_gpus() > 0:\n        print(\"Using GPU for FAISS operations\")\n        res = faiss.StandardGpuResources()\n        \n        # Create CPU index first\n        quantizer = faiss.IndexFlatIP(embedding_dimension)\n        cpu_index = faiss.IndexIVFFlat(quantizer, embedding_dimension, n_clusters, faiss.METRIC_INNER_PRODUCT)\n        \n        # Move to GPU\n        index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n    else:\n        print(\"Using CPU for FAISS operations\")\n        quantizer = faiss.IndexFlatIP(embedding_dimension)\n        index = faiss.IndexIVFFlat(quantizer, embedding_dimension, n_clusters, faiss.METRIC_INNER_PRODUCT)\n    \n    # Enhanced training parameters\n    index.nprobe = min(200, n_clusters)\n    \n    # Train with full dataset subset\n    index.train(embeddings[np.random.choice(len(embeddings), min(150000, len(embeddings)), replace=False)])\n    index.add(embeddings)\n    return index\n\ndef find_similar_trials(target_nct, embeddings_df, k=100):\n    \"\"\"Find top-k similar trials for a specific NCT number\"\"\"\n    # Locate target NCT\n    target_row = embeddings_df[embeddings_df['NCT Number'] == target_nct]\n    if target_row.empty:\n        raise ValueError(f\"NCT number {target_nct} not found in dataset\")\n    \n    # Prepare embeddings with CONTIGUOUS check\n    nct_ids = embeddings_df['NCT Number'].values\n    embeddings = embeddings_df.iloc[:, 1:769].values.astype('float32')\n    embeddings = np.ascontiguousarray(embeddings)  # ðŸ”‘ Critical fix\n    \n    # Create index (normalization happens inside create_optimized_index)\n    index = create_optimized_index(embeddings)\n    \n    # Prepare target embedding with CONTIGUOUS check\n    target_embedding = target_row.iloc[:, 1:769].values.astype('float32')\n    target_embedding = np.ascontiguousarray(target_embedding)  # ðŸ”‘ Critical fix\n    faiss.normalize_L2(target_embedding)\n    \n    # Search\n    similarities, indices = index.search(target_embedding, k + 1)\n    \n    # Process results\n    results = []\n    for idx, score in zip(indices[0], similarities[0]):\n        if nct_ids[idx] != target_nct:\n            results.append({'NCT_Number': nct_ids[idx], 'Similarity': score})\n    \n    return pd.DataFrame(results[:k])\n\n# Example usage\nif __name__ == \"__main__\":\n    embeddings_df = pd.read_csv('/kaggle/input/embeddingstrials/embeddings_output (2).csv')\n    \n    similar_trials_df = find_similar_trials(\n        target_nct=\"NCT03518073\",\n        embeddings_df=embeddings_df,\n        k=100\n    )\n    \n    similar_trials_df.to_csv(\"top_100_similar_trials3.csv\", index=False)\n    print(\"Results saved successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T17:19:51.152292Z","iopub.execute_input":"2025-01-23T17:19:51.152584Z","iopub.status.idle":"2025-01-23T17:20:08.766654Z","shell.execute_reply.started":"2025-01-23T17:19:51.152564Z","shell.execute_reply":"2025-01-23T17:20:08.765875Z"}},"outputs":[{"name":"stdout","text":"Using GPU for FAISS operations\nResults saved successfully\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}